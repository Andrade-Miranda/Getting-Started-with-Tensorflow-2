{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "data = diabetes['data']\n",
    "targets = diabetes['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(data, targets, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Callback\n",
    "##### we will use logs dictionary to access the loss and metric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_batch_end(self,batch,logs = None):\n",
    "        if batch % 2 is 0:\n",
    "            print( f\"\\n After Batch {batch}, loss is {logs['loss']}\" )\n",
    "\n",
    "    def on_test_batch_end(self,batch,logs=None):\n",
    "        if batch % 2 is 0:\n",
    "            print(f\"\\n After Batch {batch}, loss is {logs['loss']} \")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        print(f\"Epoch {epoch}, Mean Absolute Error is {logs['mae']}, Loss is {logs['loss']}\")\n",
    "        \n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        print(f\"Finished Prediction on Batch {batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After Batch 0, loss is 23378.5390625\n",
      "\n",
      " After Batch 2, loss is 26356.875\n",
      "\n",
      " After Batch 4, loss is 28064.83984375\n",
      "\n",
      " After Batch 6, loss is 28902.17578125\n",
      "Epoch 0, Mean Absolute Error is 151.17138671875, Loss is 28902.17578125\n",
      "\n",
      " After Batch 0, loss is 29013.181640625\n",
      "\n",
      " After Batch 2, loss is 27027.478515625\n",
      "\n",
      " After Batch 4, loss is 26629.837890625\n",
      "\n",
      " After Batch 6, loss is 28592.13671875\n",
      "Epoch 1, Mean Absolute Error is 150.22731018066406, Loss is 28592.13671875\n",
      "\n",
      " After Batch 0, loss is 32774.3359375\n",
      "\n",
      " After Batch 2, loss is 29210.560546875\n",
      "\n",
      " After Batch 4, loss is 28264.087890625\n",
      "\n",
      " After Batch 6, loss is 28034.705078125\n",
      "Epoch 2, Mean Absolute Error is 148.52096557617188, Loss is 28034.705078125\n",
      "\n",
      " After Batch 0, loss is 24783.283203125\n",
      "\n",
      " After Batch 2, loss is 25610.365234375\n",
      "\n",
      " After Batch 4, loss is 26349.353515625\n",
      "\n",
      " After Batch 6, loss is 27056.611328125\n",
      "Epoch 3, Mean Absolute Error is 145.59791564941406, Loss is 27056.611328125\n",
      "\n",
      " After Batch 0, loss is 23100.11328125\n",
      "\n",
      " After Batch 2, loss is 24967.482421875\n",
      "\n",
      " After Batch 4, loss is 23722.609375\n",
      "\n",
      " After Batch 6, loss is 25516.68359375\n",
      "Epoch 4, Mean Absolute Error is 140.84075927734375, Loss is 25516.68359375\n",
      "\n",
      " After Batch 0, loss is 32334.853515625\n",
      "\n",
      " After Batch 2, loss is 25075.267578125\n",
      "\n",
      " After Batch 4, loss is 23208.95703125\n",
      "\n",
      " After Batch 6, loss is 23319.591796875\n",
      "Epoch 5, Mean Absolute Error is 133.6404571533203, Loss is 23319.591796875\n",
      "\n",
      " After Batch 0, loss is 22264.55859375\n",
      "\n",
      " After Batch 2, loss is 21115.310546875\n",
      "\n",
      " After Batch 4, loss is 20324.177734375\n",
      "\n",
      " After Batch 6, loss is 20305.88671875\n",
      "Epoch 6, Mean Absolute Error is 123.1373062133789, Loss is 20305.88671875\n",
      "\n",
      " After Batch 0, loss is 18907.564453125\n",
      "\n",
      " After Batch 2, loss is 16682.220703125\n",
      "\n",
      " After Batch 4, loss is 16877.86328125\n",
      "\n",
      " After Batch 6, loss is 16886.048828125\n",
      "Epoch 7, Mean Absolute Error is 109.66046905517578, Loss is 16886.048828125\n",
      "\n",
      " After Batch 0, loss is 15242.564453125\n",
      "\n",
      " After Batch 2, loss is 15075.6015625\n",
      "\n",
      " After Batch 4, loss is 14194.1875\n",
      "\n",
      " After Batch 6, loss is 13079.87890625\n",
      "Epoch 8, Mean Absolute Error is 92.5787582397461, Loss is 13079.87890625\n",
      "\n",
      " After Batch 0, loss is 13240.67578125\n",
      "\n",
      " After Batch 2, loss is 10662.75390625\n",
      "\n",
      " After Batch 4, loss is 9824.6044921875\n",
      "\n",
      " After Batch 6, loss is 9379.8095703125\n",
      "Epoch 9, Mean Absolute Error is 76.27188873291016, Loss is 9379.8095703125\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[customCallback()], verbose = 0, batch_size=2**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After Batch 0, loss is 19756.912109375 \n",
      "\n",
      " After Batch 2, loss is 20266.62890625 \n",
      "\n",
      " After Batch 4, loss is 23119.00390625 \n"
     ]
    },
    {
     "data": {
      "text/plain": "[23119.00390625, 136.37364196777344]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val,y_val, callbacks=[customCallback()], verbose=0, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Prediction on Batch 0\n",
      "Finished Prediction on Batch 1\n",
      "Finished Prediction on Batch 2\n",
      "Finished Prediction on Batch 3\n",
      "Finished Prediction on Batch 4\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 8.376762 ],\n       [12.174041 ],\n       [22.290215 ],\n       [22.776358 ],\n       [10.463553 ],\n       [13.194703 ],\n       [38.789448 ],\n       [18.788671 ],\n       [19.576712 ],\n       [20.279705 ],\n       [37.07193  ],\n       [22.324568 ],\n       [10.980113 ],\n       [16.393162 ],\n       [24.081112 ],\n       [32.423786 ],\n       [ 9.131023 ],\n       [32.69382  ],\n       [34.415005 ],\n       [21.46859  ],\n       [35.131714 ],\n       [ 9.228954 ],\n       [ 9.9706335],\n       [24.063854 ],\n       [18.500978 ],\n       [27.93496  ],\n       [ 8.815715 ],\n       [18.43633  ],\n       [26.393787 ],\n       [18.940454 ],\n       [16.778429 ],\n       [39.354088 ],\n       [26.142286 ],\n       [27.714622 ],\n       [10.443917 ],\n       [22.157833 ],\n       [32.663357 ],\n       [17.78948  ],\n       [30.428757 ],\n       [33.36573  ],\n       [13.128178 ],\n       [10.773067 ],\n       [ 8.69389  ],\n       [33.32542  ],\n       [25.316082 ]], dtype=float32)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_val,batch_size=10, callbacks=[customCallback()], verbose=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will define a custom callback to reduce the learning rate w.r.t to # of Epochs\n",
    "\n",
    "##### It is going to have a more complex custom callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = [\n",
    "    (5,0.05), (10,0.03), (15,0.02), (20,0.01)\n",
    "]\n",
    "# we will get new learning rate using this function by comparing to list above.\n",
    "def get_new_learning_rate(epoch, lr):\n",
    "    for i in lr_schedule:\n",
    "        if epoch in i:\n",
    "            lr = i[1]\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learning_rate_scheduler( tf.keras.callbacks.Callback ):\n",
    "     def __init__(self, new_lr):\n",
    "        super(Learning_rate_scheduler, self).__init__\n",
    "        #adding new learning rate function to our callback\n",
    "        self.new_lr = new_lr\n",
    "    \n",
    "     def on_epoch_begin(self, epoch, logs=None):\n",
    "        #we will check if our optimizer has learning rate option or not\n",
    "        try:\n",
    "            curr_rate = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "\n",
    "            #calling auxillary function to get scheduled learning rate, we have passed the function as parameter which is new_lr\n",
    "\n",
    "            scheduled_rate = self.new_lr(epoch, curr_rate)\n",
    "\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n",
    "\n",
    "            print(f\"Learning Rate for Epoch {epoch} is {tf.keras.backend.get_value(self.model.optimizer.lr)}\")\n",
    "\n",
    "        except Exception as E:\n",
    "            print(f'{E}\\n Most Probably your optimizer do not have learing rate option.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',\n",
    "                optimizer=\"adam\",\n",
    "                metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate for Epoch 0 is 0.0010000000474974513\n",
      "Learning Rate for Epoch 1 is 0.0010000000474974513\n",
      "Learning Rate for Epoch 2 is 0.0010000000474974513\n",
      "Learning Rate for Epoch 3 is 0.0010000000474974513\n",
      "Learning Rate for Epoch 4 is 0.0010000000474974513\n",
      "Learning Rate for Epoch 5 is 0.05000000074505806\n",
      "Learning Rate for Epoch 6 is 0.05000000074505806\n",
      "Learning Rate for Epoch 7 is 0.05000000074505806\n",
      "Learning Rate for Epoch 8 is 0.05000000074505806\n",
      "Learning Rate for Epoch 9 is 0.05000000074505806\n",
      "Learning Rate for Epoch 10 is 0.029999999329447746\n",
      "Learning Rate for Epoch 11 is 0.029999999329447746\n",
      "Learning Rate for Epoch 12 is 0.029999999329447746\n",
      "Learning Rate for Epoch 13 is 0.029999999329447746\n",
      "Learning Rate for Epoch 14 is 0.029999999329447746\n",
      "Learning Rate for Epoch 15 is 0.019999999552965164\n",
      "Learning Rate for Epoch 16 is 0.019999999552965164\n",
      "Learning Rate for Epoch 17 is 0.019999999552965164\n",
      "Learning Rate for Epoch 18 is 0.019999999552965164\n",
      "Learning Rate for Epoch 19 is 0.019999999552965164\n",
      "Learning Rate for Epoch 20 is 0.009999999776482582\n",
      "Learning Rate for Epoch 21 is 0.009999999776482582\n",
      "Learning Rate for Epoch 22 is 0.009999999776482582\n",
      "Learning Rate for Epoch 23 is 0.009999999776482582\n",
      "Learning Rate for Epoch 24 is 0.009999999776482582\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2219cc18c48>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=25, batch_size=64, callbacks=[Learning_rate_scheduler(get_new_learning_rate)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitmyenvconda4a11ba26287d4d1c969b9946e31eb2a2",
   "language": "python",
   "display_name": "Python 3.7.7 64-bit ('myenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}